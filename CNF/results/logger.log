2022-03-14 10:16:31,069 Namespace(activation='tanh', batch_size=32, clamping_epsilon=1e-06, dataset_file='../datasets/grid-dataset.txt', hidden_layers=1, hidden_nodes=[100], initialization='standard-normal', learning_rate=0.0005, lr_epochs=10, lr_mult_fact=1.0, model_structure='clamp-zero', optimizer='sgd', parameterization='normal', plot_change_in_w_b=True, results_file='results/', seed=18, std_a=0.005, total_epochs=200, train_last_layer=True)
2022-03-14 10:16:31,070 cpu
2022-03-14 10:16:32,685 Epoch number: -1.000000
 Train loss: 10.677451
 Train probability loss: 1.838107
 Train Jacobian loss: 8.839343
 Running average loss: 0.000000


2022-03-14 10:16:33,506 Epoch number: 0.000000
 Train loss: 5.300446
 Train probability loss: 1.889186
 Train Jacobian loss: 3.411260
 Running average loss: 6.242753
 Learning rate: 0.000500 


2022-03-14 10:16:34,179 Epoch number: 1.000000
 Train loss: 4.683289
 Train probability loss: 1.937293
 Train Jacobian loss: 2.745995
 Running average loss: 5.598739
 Learning rate: 0.000500 


2022-03-14 10:16:34,837 Epoch number: 2.000000
 Train loss: 4.348125
 Train probability loss: 1.982707
 Train Jacobian loss: 2.365418
 Running average loss: 5.233627
 Learning rate: 0.000500 


2022-03-14 10:16:35,495 Epoch number: 3.000000
 Train loss: 4.127304
 Train probability loss: 2.025551
 Train Jacobian loss: 2.101752
 Running average loss: 4.983158
 Learning rate: 0.000500 


2022-03-14 10:16:36,152 Epoch number: 4.000000
 Train loss: 3.968118
 Train probability loss: 2.065993
 Train Jacobian loss: 1.902125
 Running average loss: 4.795383
 Learning rate: 0.000500 


2022-03-14 10:16:36,809 Epoch number: 5.000000
 Train loss: 3.847103
 Train probability loss: 2.104173
 Train Jacobian loss: 1.742930
 Running average loss: 4.647058
 Learning rate: 0.000500 


